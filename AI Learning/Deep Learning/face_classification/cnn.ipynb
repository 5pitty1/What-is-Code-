{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2zG_gzb9nXB"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yNLkDijm9oJF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA NOT supported\n",
      "Nnet(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 21, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(21, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Conv2d(20, 15, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(15, 7, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1575, out_features=300, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=300, out_features=201, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "net=Nnet().to(computing_device)\n",
    "net.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(net)\n",
    "\n",
    "#loss criteria are defined in the torch.nn package\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = optim.Adam(net.parameters(),lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256), transforms.ToTensor()])\n",
    "dataset = loader('train.csv', \"\", transform=transform)\n",
    "batch_size = 64\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadStatistics(labels, outputs):\n",
    "    decisions = outputs.argmax(axis=1)\n",
    "\n",
    "    for i in range (1,201):\n",
    "        truePositives[i-1] += int(torch.sum((decisions == i) & (decisions == labels)))\n",
    "        falsePositives[i-1] += int(torch.sum((decisions == i) & (decisions != labels)))\n",
    "        falseNegatives[i-1] += int(torch.sum((labels == i) & (decisions != labels)))\n",
    "\n",
    "\n",
    "def printStatistics():\n",
    "    acc = accuracy(truePositives, batch_size)\n",
    "    prec = precision(truePositives, falsePositives)\n",
    "    rec = recall(truePositives, falseNegatives)\n",
    "    bal = bcr(prec, rec)\n",
    "\n",
    "    print(\"True Positives: \", truePositives)    \n",
    "    print(\"False Positives: \", falsePositives)\n",
    "    print(\"False Negatives: \", falseNegatives)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print(\"Precision: \", prec)\n",
    "    print(\"Recall: \", rec)\n",
    "    print(\"Balanced Classification Rate: \", bal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_batch 0\n",
      "mini_batch 1\n",
      "mini_batch 2\n",
      "mini_batch 3\n",
      "mini_batch 4\n",
      "mini_batch 5\n",
      "mini_batch 6\n",
      "mini_batch 7\n",
      "mini_batch 8\n",
      "mini_batch 9\n",
      "mini_batch 10\n",
      "mini_batch 11\n",
      "mini_batch 12\n",
      "mini_batch 13\n",
      "mini_batch 14\n",
      "mini_batch 15\n",
      "mini_batch 16\n",
      "mini_batch 17\n",
      "mini_batch 18\n",
      "mini_batch 19\n",
      "mini_batch 20\n",
      "mini_batch 21\n",
      "mini_batch 22\n",
      "mini_batch 23\n",
      "mini_batch 24\n",
      "mini_batch 25\n",
      "mini_batch 26\n",
      "mini_batch 27\n",
      "mini_batch 28\n",
      "mini_batch 29\n",
      "mini_batch 30\n",
      "mini_batch 31\n",
      "mini_batch 32\n",
      "mini_batch 33\n",
      "mini_batch 34\n",
      "mini_batch 35\n",
      "mini_batch 36\n",
      "mini_batch 37\n",
      "mini_batch 38\n",
      "mini_batch 39\n",
      "mini_batch 40\n",
      "mini_batch 41\n",
      "mini_batch 42\n",
      "mini_batch 43\n",
      "mini_batch 44\n",
      "mini_batch 45\n",
      "mini_batch 46\n",
      "mini_batch 47\n",
      "mini_batch 48\n",
      "mini_batch 49\n",
      "Epoch 1\n",
      "average minibatch 50 loss: 5.232\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "printStatistics() takes 0 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a28644f005f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'average minibatch %d loss: %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mminibatch_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_minibatch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mprintStatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# Add the averaged loss over N minibatches and reset the counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: printStatistics() takes 0 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "N = 50\n",
    "\n",
    "validation_loss = []\n",
    "\n",
    "truePositives = np.zeros(200)\n",
    "falsePositives = np.zeros(200)\n",
    "falseNegatives = np.zeros(200)\n",
    "\n",
    "for epoch in range(50):\n",
    "    N_minibatch_loss = 0.0\n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "        print(\"mini_batch\", minibatch_count)\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = net(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "                    \n",
    "                \n",
    "        if minibatch_count % N == N-1:\n",
    "            #Print the loss averaged over the last N mini-batches\n",
    "            N_minibatch_loss /= N\n",
    "            print('Epoch %d' % (epoch + 1))\n",
    "            print('average minibatch %d loss: %.3f' % (minibatch_count+1, N_minibatch_loss))\n",
    "            print('')\n",
    "            printStatistics(labels, outputs)\n",
    "\n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "\n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\")\n",
    "    # TODO: Implement validation #with torch.no_grad():\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0\n",
    "        for minibatch_count, (images, labels) in enumerate(validation_loader, 0):\n",
    "            print(\"Validation mini_batch\", minibatch_count)\n",
    "            images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            valid_loss += criterion(outputs, labels)\n",
    "        validation_loss.append(valid_loss)\n",
    "        \n",
    "    if epoch >= 5:\n",
    "        early_stop = True\n",
    "        for i in range(5):\n",
    "            if validation_loss[epoch - i] < validation_loss[epoch - i - 1]:\n",
    "                early_stop = False\n",
    "        \n",
    "        if early_stop == True:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load up the test dataset\n",
    "test_dataset = loader('test.csv',transform=transform)\n",
    "test_size = len(test_dataset)\n",
    "test_indices = list(range(test_size))\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, \n",
    "                                           sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation metrics\n",
    "total_test_loss = []\n",
    "N = 50\n",
    "\n",
    "truePositives = np.zeros(200)\n",
    "falsePositives = np.zeros(200)\n",
    "falseNegatives = np.zeros(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_batch 0\n",
      "mini_batch 1\n",
      "mini_batch 2\n",
      "mini_batch 3\n",
      "mini_batch 4\n",
      "mini_batch 5\n",
      "mini_batch 6\n",
      "mini_batch 7\n",
      "mini_batch 8\n",
      "mini_batch 9\n",
      "mini_batch 10\n",
      "mini_batch 11\n",
      "mini_batch 12\n",
      "mini_batch 13\n",
      "mini_batch 14\n",
      "mini_batch 15\n",
      "mini_batch 16\n",
      "mini_batch 17\n",
      "mini_batch 18\n",
      "mini_batch 19\n",
      "mini_batch 20\n",
      "mini_batch 21\n",
      "mini_batch 22\n",
      "mini_batch 23\n",
      "mini_batch 24\n",
      "mini_batch 25\n",
      "mini_batch 26\n",
      "mini_batch 27\n",
      "mini_batch 28\n",
      "mini_batch 29\n",
      "mini_batch 30\n",
      "mini_batch 31\n",
      "mini_batch 32\n",
      "mini_batch 33\n",
      "mini_batch 34\n",
      "mini_batch 35\n",
      "mini_batch 36\n",
      "mini_batch 37\n",
      "mini_batch 38\n",
      "mini_batch 39\n",
      "mini_batch 40\n",
      "mini_batch 41\n",
      "mini_batch 42\n",
      "mini_batch 43\n",
      "mini_batch 44\n",
      "mini_batch 45\n",
      "mini_batch 46\n",
      "mini_batch 47\n",
      "mini_batch 48\n",
      "mini_batch 49\n",
      "mini_batch 50\n",
      "True Positives:  [19.  1.  9.  4. 10.  7. 11.  4. 16.  8.  4.  4. 13.  3. 12. 11.  8.  7.\n",
      "  6.  8.  8.  6.  5. 17.  9.  1.  7. 12. 21.  6.  5.  6. 14.  5. 20. 13.\n",
      "  8. 11.  6.  8.  7.  2.  4.  7.  4.  8.  7. 11.  5. 10.  5.  6. 13.  9.\n",
      "  3.  4.  6. 20. 12.  5.  1. 14.  7.  9. 15.  9. 11. 14. 18.  2.  7.  7.\n",
      "  1. 11. 12.  7.  4. 16.  4.  1. 15. 14.  9.  3. 15.  6.  8.  1.  9. 10.\n",
      "  4.  4. 11. 11. 13.  0.  3.  8.  5. 22.  9. 13. 11.  2. 18.  9. 12.  8.\n",
      " 24. 10.  8.  9.  5. 18.  3.  9.  8. 20. 12.  4.  8. 13. 12.  0. 13.  4.\n",
      " 12.  8.  6.  5.  4. 10.  7. 13.  7. 13. 15.  7.  6.  4.  9.  1.  9.  5.\n",
      " 10. 19.  8.  9. 17.  1. 17.  8. 15. 11. 10.  1. 15.  7.  9.  5.  5.  6.\n",
      "  9. 12.  2.  4.  7. 14. 15.  8. 17. 13. 15.  5.  8.  8. 18.  4. 23. 17.\n",
      "  6.  4.  5. 15.  9. 15.  2.  5.  9. 14.  3. 12.  2. 16.  7.  4.  4. 12.\n",
      "  6. 14.]\n",
      "False Positives:  [29.  1.  5.  6. 17.  1.  2.  9.  4. 28.  2.  1.  2.  3.  4.  6.  4.  1.\n",
      "  0.  0.  6.  5. 11. 22. 10.  3. 18.  5. 10.  7.  3.  0. 42.  9. 10.  4.\n",
      " 14. 11.  1. 15.  2.  2.  0.  2.  2.  2.  0.  3.  5.  8. 18.  4.  4.  1.\n",
      "  3. 12.  7.  4. 12. 15.  3.  5.  4.  7. 26. 13.  5.  2.  9.  3.  0.  6.\n",
      "  0.  5.  9. 10.  1. 14. 16.  2.  9.  2.  9.  2. 11.  4.  1.  9.  4.  3.\n",
      "  2.  6. 14.  7.  4.  1.  1.  2.  5.  2.  8.  9. 15.  4. 27. 13.  2.  4.\n",
      " 36. 18.  5. 13.  1. 11.  7.  4.  4.  6.  3.  9.  1. 24. 16.  3.  6.  3.\n",
      " 13.  3.  6.  1. 11.  3.  7. 12.  4.  7.  5. 12.  3.  1.  2.  2.  6. 12.\n",
      "  3. 14.  5.  1.  9.  1.  2.  8.  3.  8.  2.  3. 15. 13.  5.  0.  5. 24.\n",
      "  5. 10.  1.  1. 20.  6.  9. 11.  2. 11. 16.  2.  3.  8. 10.  2. 11.  1.\n",
      "  7.  7.  2. 11.  8.  5.  1.  7.  3. 18.  3. 32.  0.  5.  4.  4.  5. 23.\n",
      "  0. 23.]\n",
      "False Negatives:  [13.  8.  5. 11.  3.  4. 14.  6. 11.  3.  4.  9. 15.  3.  7.  4.  4. 15.\n",
      "  7.  8. 23. 26. 16. 12.  8.  4.  9.  6.  4.  3.  5. 12.  8.  8.  7. 11.\n",
      " 22.  7.  6.  7.  6.  9.  8.  2. 11.  6.  3.  5.  5.  4. 11.  7.  7.  5.\n",
      "  4.  5.  6.  4.  7.  9. 10.  5. 15. 12.  5.  8. 19.  4.  2.  7. 17. 12.\n",
      "  6.  4.  8.  6.  5.  1.  6.  5.  3.  6.  4.  5.  7. 14. 11. 12. 16.  3.\n",
      "  3.  6. 18.  4.  8.  9.  5.  7.  4.  5. 14. 11.  4.  5. 13.  9.  7.  2.\n",
      "  6. 11.  4.  4.  9.  3. 10.  6.  5.  3.  7.  8.  6.  6.  2. 13.  9. 12.\n",
      "  8.  7.  8.  6.  8. 12.  1.  1.  4.  3.  5.  4.  4.  6. 11. 12.  1.  5.\n",
      "  3.  7.  3.  7.  4.  5.  7.  5.  9.  9. 16.  5.  0.  7.  9.  7. 11.  2.\n",
      "  9. 15.  4.  6.  3.  3. 10.  9.  2.  8. 11.  7.  6.  3.  4.  5.  7. 11.\n",
      "  4.  7.  1.  8.  7.  5.  9. 11.  6.  9. 18. 14.  5.  2.  4.  4.  5. 10.\n",
      "  0.  3.]\n",
      "Accuracy:  [0.296875 0.015625 0.140625 0.0625   0.15625  0.109375 0.171875 0.0625\n",
      " 0.25     0.125    0.0625   0.0625   0.203125 0.046875 0.1875   0.171875\n",
      " 0.125    0.109375 0.09375  0.125    0.125    0.09375  0.078125 0.265625\n",
      " 0.140625 0.015625 0.109375 0.1875   0.328125 0.09375  0.078125 0.09375\n",
      " 0.21875  0.078125 0.3125   0.203125 0.125    0.171875 0.09375  0.125\n",
      " 0.109375 0.03125  0.0625   0.109375 0.0625   0.125    0.109375 0.171875\n",
      " 0.078125 0.15625  0.078125 0.09375  0.203125 0.140625 0.046875 0.0625\n",
      " 0.09375  0.3125   0.1875   0.078125 0.015625 0.21875  0.109375 0.140625\n",
      " 0.234375 0.140625 0.171875 0.21875  0.28125  0.03125  0.109375 0.109375\n",
      " 0.015625 0.171875 0.1875   0.109375 0.0625   0.25     0.0625   0.015625\n",
      " 0.234375 0.21875  0.140625 0.046875 0.234375 0.09375  0.125    0.015625\n",
      " 0.140625 0.15625  0.0625   0.0625   0.171875 0.171875 0.203125 0.\n",
      " 0.046875 0.125    0.078125 0.34375  0.140625 0.203125 0.171875 0.03125\n",
      " 0.28125  0.140625 0.1875   0.125    0.375    0.15625  0.125    0.140625\n",
      " 0.078125 0.28125  0.046875 0.140625 0.125    0.3125   0.1875   0.0625\n",
      " 0.125    0.203125 0.1875   0.       0.203125 0.0625   0.1875   0.125\n",
      " 0.09375  0.078125 0.0625   0.15625  0.109375 0.203125 0.109375 0.203125\n",
      " 0.234375 0.109375 0.09375  0.0625   0.140625 0.015625 0.140625 0.078125\n",
      " 0.15625  0.296875 0.125    0.140625 0.265625 0.015625 0.265625 0.125\n",
      " 0.234375 0.171875 0.15625  0.015625 0.234375 0.109375 0.140625 0.078125\n",
      " 0.078125 0.09375  0.140625 0.1875   0.03125  0.0625   0.109375 0.21875\n",
      " 0.234375 0.125    0.265625 0.203125 0.234375 0.078125 0.125    0.125\n",
      " 0.28125  0.0625   0.359375 0.265625 0.09375  0.0625   0.078125 0.234375\n",
      " 0.140625 0.234375 0.03125  0.078125 0.140625 0.21875  0.046875 0.1875\n",
      " 0.03125  0.25     0.109375 0.0625   0.0625   0.1875   0.09375  0.21875 ]\n",
      "Precision:  [0.39583333 0.5        0.64285714 0.4        0.37037037 0.875\n",
      " 0.84615385 0.30769231 0.8        0.22222222 0.66666667 0.8\n",
      " 0.86666667 0.5        0.75       0.64705882 0.66666667 0.875\n",
      " 1.         1.         0.57142857 0.54545455 0.3125     0.43589744\n",
      " 0.47368421 0.25       0.28       0.70588235 0.67741935 0.46153846\n",
      " 0.625      1.         0.25       0.35714286 0.66666667 0.76470588\n",
      " 0.36363636 0.5        0.85714286 0.34782609 0.77777778 0.5\n",
      " 1.         0.77777778 0.66666667 0.8        1.         0.78571429\n",
      " 0.5        0.55555556 0.2173913  0.6        0.76470588 0.9\n",
      " 0.5        0.25       0.46153846 0.83333333 0.5        0.25\n",
      " 0.25       0.73684211 0.63636364 0.5625     0.36585366 0.40909091\n",
      " 0.6875     0.875      0.66666667 0.4        1.         0.53846154\n",
      " 1.         0.6875     0.57142857 0.41176471 0.8        0.53333333\n",
      " 0.2        0.33333333 0.625      0.875      0.5        0.6\n",
      " 0.57692308 0.6        0.88888889 0.1        0.69230769 0.76923077\n",
      " 0.66666667 0.4        0.44       0.61111111 0.76470588 0.\n",
      " 0.75       0.8        0.5        0.91666667 0.52941176 0.59090909\n",
      " 0.42307692 0.33333333 0.4        0.40909091 0.85714286 0.66666667\n",
      " 0.4        0.35714286 0.61538462 0.40909091 0.83333333 0.62068966\n",
      " 0.3        0.69230769 0.66666667 0.76923077 0.8        0.30769231\n",
      " 0.88888889 0.35135135 0.42857143 0.         0.68421053 0.57142857\n",
      " 0.48       0.72727273 0.5        0.83333333 0.26666667 0.76923077\n",
      " 0.5        0.52       0.63636364 0.65       0.75       0.36842105\n",
      " 0.66666667 0.8        0.81818182 0.33333333 0.6        0.29411765\n",
      " 0.76923077 0.57575758 0.61538462 0.9        0.65384615 0.5\n",
      " 0.89473684 0.5        0.83333333 0.57894737 0.83333333 0.25\n",
      " 0.5        0.35       0.64285714 1.         0.5        0.2\n",
      " 0.64285714 0.54545455 0.66666667 0.8        0.25925926 0.7\n",
      " 0.625      0.42105263 0.89473684 0.54166667 0.48387097 0.71428571\n",
      " 0.72727273 0.5        0.64285714 0.66666667 0.67647059 0.94444444\n",
      " 0.46153846 0.36363636 0.71428571 0.57692308 0.52941176 0.75\n",
      " 0.66666667 0.41666667 0.75       0.4375     0.5        0.27272727\n",
      " 1.         0.76190476 0.63636364 0.5        0.44444444 0.34285714\n",
      " 1.         0.37837838]\n",
      "Recall:  [0.59375    0.11111111 0.64285714 0.26666667 0.76923077 0.63636364\n",
      " 0.44       0.4        0.59259259 0.72727273 0.5        0.30769231\n",
      " 0.46428571 0.5        0.63157895 0.73333333 0.66666667 0.31818182\n",
      " 0.46153846 0.5        0.25806452 0.1875     0.23809524 0.5862069\n",
      " 0.52941176 0.2        0.4375     0.66666667 0.84       0.66666667\n",
      " 0.5        0.33333333 0.63636364 0.38461538 0.74074074 0.54166667\n",
      " 0.26666667 0.61111111 0.5        0.53333333 0.53846154 0.18181818\n",
      " 0.33333333 0.77777778 0.26666667 0.57142857 0.7        0.6875\n",
      " 0.5        0.71428571 0.3125     0.46153846 0.65       0.64285714\n",
      " 0.42857143 0.44444444 0.5        0.83333333 0.63157895 0.35714286\n",
      " 0.09090909 0.73684211 0.31818182 0.42857143 0.75       0.52941176\n",
      " 0.36666667 0.77777778 0.9        0.22222222 0.29166667 0.36842105\n",
      " 0.14285714 0.73333333 0.6        0.53846154 0.44444444 0.94117647\n",
      " 0.4        0.16666667 0.83333333 0.7        0.69230769 0.375\n",
      " 0.68181818 0.3        0.42105263 0.07692308 0.36       0.76923077\n",
      " 0.57142857 0.4        0.37931034 0.73333333 0.61904762 0.\n",
      " 0.375      0.53333333 0.55555556 0.81481481 0.39130435 0.54166667\n",
      " 0.73333333 0.28571429 0.58064516 0.5        0.63157895 0.8\n",
      " 0.8        0.47619048 0.66666667 0.69230769 0.35714286 0.85714286\n",
      " 0.23076923 0.6        0.61538462 0.86956522 0.63157895 0.33333333\n",
      " 0.57142857 0.68421053 0.85714286 0.         0.59090909 0.25\n",
      " 0.6        0.53333333 0.42857143 0.45454545 0.33333333 0.45454545\n",
      " 0.875      0.92857143 0.63636364 0.8125     0.75       0.63636364\n",
      " 0.6        0.4        0.45       0.07692308 0.9        0.5\n",
      " 0.76923077 0.73076923 0.72727273 0.5625     0.80952381 0.16666667\n",
      " 0.70833333 0.61538462 0.625      0.55       0.38461538 0.16666667\n",
      " 1.         0.5        0.5        0.41666667 0.3125     0.75\n",
      " 0.5        0.44444444 0.33333333 0.4        0.7        0.82352941\n",
      " 0.6        0.47058824 0.89473684 0.61904762 0.57692308 0.41666667\n",
      " 0.57142857 0.72727273 0.81818182 0.44444444 0.76666667 0.60714286\n",
      " 0.6        0.36363636 0.83333333 0.65217391 0.5625     0.75\n",
      " 0.18181818 0.3125     0.6        0.60869565 0.14285714 0.46153846\n",
      " 0.28571429 0.88888889 0.63636364 0.5        0.44444444 0.54545455\n",
      " 1.         0.82352941]\n",
      "Balanced Classification Rate:  [0.49479167 0.30555556 0.64285714 0.33333333 0.56980057 0.75568182\n",
      " 0.64307692 0.35384615 0.6962963  0.47474747 0.58333333 0.55384615\n",
      " 0.66547619 0.5        0.69078947 0.69019608 0.66666667 0.59659091\n",
      " 0.73076923 0.75       0.41474654 0.36647727 0.27529762 0.51105217\n",
      " 0.50154799 0.225      0.35875    0.68627451 0.75870968 0.56410256\n",
      " 0.5625     0.66666667 0.44318182 0.37087912 0.7037037  0.65318627\n",
      " 0.31515152 0.55555556 0.67857143 0.44057971 0.65811966 0.34090909\n",
      " 0.66666667 0.77777778 0.46666667 0.68571429 0.85       0.73660714\n",
      " 0.5        0.63492063 0.26494565 0.53076923 0.70735294 0.77142857\n",
      " 0.46428571 0.34722222 0.48076923 0.83333333 0.56578947 0.30357143\n",
      " 0.17045455 0.73684211 0.47727273 0.49553571 0.55792683 0.46925134\n",
      " 0.52708333 0.82638889 0.78333333 0.31111111 0.64583333 0.4534413\n",
      " 0.57142857 0.71041667 0.58571429 0.47511312 0.62222222 0.7372549\n",
      " 0.3        0.25       0.72916667 0.7875     0.59615385 0.4875\n",
      " 0.62937063 0.45       0.65497076 0.08846154 0.52615385 0.76923077\n",
      " 0.61904762 0.4        0.40965517 0.67222222 0.69187675 0.\n",
      " 0.5625     0.66666667 0.52777778 0.86574074 0.46035806 0.56628788\n",
      " 0.57820513 0.30952381 0.49032258 0.45454545 0.7443609  0.73333333\n",
      " 0.6        0.41666667 0.64102564 0.5506993  0.5952381  0.73891626\n",
      " 0.26538462 0.64615385 0.64102564 0.81939799 0.71578947 0.32051282\n",
      " 0.73015873 0.51778094 0.64285714 0.         0.63755981 0.41071429\n",
      " 0.54       0.63030303 0.46428571 0.64393939 0.3        0.61188811\n",
      " 0.6875     0.72428571 0.63636364 0.73125    0.75       0.50239234\n",
      " 0.63333333 0.6        0.63409091 0.20512821 0.75       0.39705882\n",
      " 0.76923077 0.6532634  0.67132867 0.73125    0.73168498 0.33333333\n",
      " 0.80153509 0.55769231 0.72916667 0.56447368 0.60897436 0.20833333\n",
      " 0.75       0.425      0.57142857 0.70833333 0.40625    0.475\n",
      " 0.57142857 0.49494949 0.5        0.6        0.47962963 0.76176471\n",
      " 0.6125     0.44582043 0.89473684 0.58035714 0.53039702 0.56547619\n",
      " 0.64935065 0.61363636 0.73051948 0.55555556 0.72156863 0.77579365\n",
      " 0.53076923 0.36363636 0.77380952 0.61454849 0.54595588 0.75\n",
      " 0.42424242 0.36458333 0.675      0.52309783 0.32142857 0.36713287\n",
      " 0.64285714 0.82539683 0.63636364 0.5        0.44444444 0.44415584\n",
      " 1.         0.6009539 ]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(test_loader, 0):\n",
    "        print(\"mini_batch\", minibatch_count)\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = net(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_test_loss.append(loss.item())\n",
    "\n",
    "        loadStatistics(labels, outputs)\n",
    "        \n",
    "    printStatistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "cnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
